{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbbcb80c-b34e-4df5-a5a2-678261bd2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HUB = dict()\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "DATA_HUB['SNLI'] = ('https://nlp.stanford.edu/projects/snli/snli_1.0.zip', '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
    "DATA_HUB['glove.6b.50d'] = (DATA_URL + 'glove.6B.50d.zip', '0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n",
    "DATA_HUB['glove.6b.100d'] = (DATA_URL + 'glove.6B.100d.zip', 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n",
    "DATA_HUB['glove.42b.300d'] = (DATA_URL + 'glove.42B.300d.zip', 'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n",
    "DATA_HUB['bert.small'] = (DATA_URL + 'bert.small.torch.zip', 'c72329e68a732bef0452e4b96a1c341c8910f81f')\n",
    "DATA_HUB['bert.base'] = (DATA_URL + 'bert.base.torch.zip', '225d66f04cae318b841a13d32af3acc165f253ac')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "\n",
    "nn_Module = nn.Module\n",
    "\n",
    "import json\n",
    "import multiprocessing\n",
    "import collections\n",
    "import hashlib\n",
    "import inspect\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import requests\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as tkr\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib_inline import backend_inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from scipy.spatial import distance_matrix\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9d6dbd-2bbf-4072-b778-e4a024240af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlinumpy = lambda x, *args, **kwargs: x.detach().numpy(*args, **kwargs)\n",
    "nlito = lambda x, *args, **kwargs: x.to(*args, **kwargs)\n",
    "size = lambda x, *args, **kwargs: x.numel(*args, **kwargs)\n",
    "argmax = lambda x, *args, **kwargs: x.argmax(*args, **kwargs)\n",
    "astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\n",
    "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7c2cc40-35f3-44d1-b315-b3cb01aaec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_workers():\n",
    "    \"\"\"Use 4 processes to read the data.\"\"\"\n",
    "    return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c1e1c8-c986-48d9-9b85-7c1c8d1aa59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, folder='../data', sha1_hash=None):\n",
    "    \"\"\"Download a file to folder and return the local filepath.\"\"\"\n",
    "    if not url.startswith('http'):\n",
    "        # For back compatability\n",
    "        url, sha1_hash = DATA_HUB[url]\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    fname = os.path.join(folder, url.split('/')[-1])\n",
    "    # Check if hit cache\n",
    "    if os.path.exists(fname) and sha1_hash:\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    # Download\n",
    "    print(f' ---> Downloaded {fname} from {url}.')\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6483676-c4c6-449d-8511-a52b92493cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_snli(fname):\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    target_dir = os.path.join(base_dir, 'snli_1.0')\n",
    "    if os.path.exists(target_dir):\n",
    "        print(f' ---> snli data already extracted at {target_dir}')\n",
    "        return\n",
    "    os.makedirs(target_dir,exist_ok=True)\n",
    "    with zipfile.ZipFile(fname, 'r') as input:\n",
    "        for item in input.namelist():\n",
    "            if '__MACOSX' in item:\n",
    "                continue\n",
    "            if '.DS_Store' in item:\n",
    "                continue\n",
    "            file_name = Path(item).name\n",
    "            if 'Icon' in file_name:\n",
    "                continue\n",
    "            source = input.open(item)\n",
    "            target = open(os.path.join(target_dir, file_name), \"wb\")\n",
    "            with source, target:\n",
    "                shutil.copyfileobj(source, target)\n",
    "    print(f' ---> extracted snli data to {target_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4578560c-b26b-49bb-8916-73eb6c7db9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_extract(name, folder=None):\n",
    "    \"\"\"Download and extract a zip/tar file.\"\"\"\n",
    "    \n",
    "    fname = download(name)\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    data_dir, ext = os.path.splitext(fname)\n",
    "    if ext == '.zip':\n",
    "        if 'snli_1.0.zip' in fname:\n",
    "            extract_snli(fname)\n",
    "        else:\n",
    "            fp = zipfile.ZipFile(fname, 'r')\n",
    "            fp.extractall(base_dir)\n",
    "    elif ext in ('.tar', '.gz'):\n",
    "        fp = tarfile.open(fname, 'r')\n",
    "        fp.extractall(base_dir)\n",
    "    else:\n",
    "        assert False, 'Only zip/tar files can be extracted.'\n",
    "    return os.path.join(base_dir, folder) if folder else data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19de1583-2d64-48d9-af5b-5b6a98ecf0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_snli(data_dir, is_train):\n",
    "    \"\"\"Read the SNLI dataset into premises, hypotheses, and labels.`\"\"\"\n",
    "    \n",
    "    def extract_text(s):\n",
    "        # Remove information that will not be used by us\n",
    "        s = re.sub('\\\\(', '', s)\n",
    "        s = re.sub('\\\\)', '', s)\n",
    "        # Substitute two or more consecutive whitespace with space\n",
    "        s = re.sub('\\\\s{2,}', ' ', s)\n",
    "        return s.strip()\n",
    "    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt'\n",
    "                             if is_train else 'snli_1.0_test.txt')\n",
    "    with open(file_name, 'r') as f:\n",
    "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
    "    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n",
    "    hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]\n",
    "    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n",
    "    return premises, hypotheses, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f34964-7cf3-4730-a684-6bab7299d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    \"\"\"Split text lines into word or character tokens.`\"\"\"\n",
    "    \n",
    "    assert token in ('word', 'char'), 'Unknown token type: ' + token\n",
    "    return [line.split() if token == 'word' else list(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b60efd5-e2d4-4c0f-832b-b2d6e45b2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        # Flatten a 2D list if needed\n",
    "        if tokens and isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        # Count token frequencies\n",
    "        counter = collections.Counter(tokens)\n",
    "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                  reverse=True)\n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
    "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
    "            return [self.idx_to_token[int(index)] for index in indices]\n",
    "        return self.idx_to_token[indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return self.token_to_idx['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "479bc3d9-51a0-421c-b26a-17432275ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_pad(line, num_steps, padding_token):\n",
    "    \"\"\"Truncate or pad sequences.\"\"\"\n",
    "    if len(line) > num_steps:\n",
    "        return line[:num_steps]  # Truncate\n",
    "    return line + [padding_token] * (num_steps - len(line))  # Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad122cb6-8967-42ff-b87d-c5e708b97824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A customized dataset to load the SNLI dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, num_steps, vocab=None):\n",
    "        self.num_steps = num_steps\n",
    "        all_premise_tokens = tokenize(dataset[0])\n",
    "        all_hypothesis_tokens = tokenize(dataset[1])\n",
    "        if vocab is None:\n",
    "            self.vocab = Vocab(all_premise_tokens + all_hypothesis_tokens,\n",
    "                                   min_freq=5, reserved_tokens=['<pad>'])\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        self.premises = self._pad(all_premise_tokens)\n",
    "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        print(' ---> read ' + str(len(self.premises)) + ' examples')\n",
    "\n",
    "    def _pad(self, lines):\n",
    "        return torch.tensor([truncate_pad(\n",
    "            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n",
    "                         for line in lines])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.premises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f644c497-a60b-421d-8519-d80951bd5711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_snli(batch_size, num_steps=50):\n",
    "    \"\"\"Download the SNLI dataset and return data iterators and vocabulary.\"\"\"\n",
    "    num_workers = get_dataloader_workers()\n",
    "    data_dir = download_extract('SNLI')\n",
    "    train_data = read_snli(data_dir, True)\n",
    "    test_data = read_snli(data_dir, False)\n",
    "    train_set = SNLIDataset(train_data, num_steps)\n",
    "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n",
    "                                             shuffle=True,\n",
    "                                             num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            num_workers=num_workers)\n",
    "    return train_iter, test_iter, train_set.vocab, data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75811314-6b0a-4710-a5f4-6f54e4556ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu():\n",
    "    \"\"\"Get the CPU device.\"\"\"\n",
    "    return torch.device('cpu')\n",
    "    \n",
    "def gpu(i=0, gpu_type=None):\n",
    "    \"\"\"Get a GPU device.\"\"\"\n",
    "    if gpu_type:\n",
    "        if gpu_type == \"cuda\":\n",
    "            return torch.device(f'cuda:{i}')\n",
    "        elif gpu_type == \"mps\":\n",
    "            return torch.device(f'mps:{i}')\n",
    "    # might not reach here, but in case\n",
    "    return cpu()\n",
    "\n",
    "def num_gpus():\n",
    "    \"\"\"Get the number of available GPUs.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        count = torch.cuda.device_count()\n",
    "        print(f' ---> will run on nividia cuda gpu(s) count {count}')\n",
    "        return \"cuda\", count\n",
    "    elif torch.backends.mps.is_available():\n",
    "        count = torch.mps.device_count()\n",
    "        print(f' ---> will run on mps gpu(s) count {count}')\n",
    "        return \"mps\", count\n",
    "    else:\n",
    "        print(f' ---> No gpus found. will run on device {cpu()}.')\n",
    "        return None, 0\n",
    "        \n",
    "def try_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\n",
    "    gpu_type, count = num_gpus()\n",
    "    if gpu_type == None:\n",
    "        return cpu()\n",
    "    if count >= i + 1:\n",
    "        return gpu(i, gpu_type)\n",
    "    return cpu()\n",
    "    \n",
    "def try_all_gpus():\n",
    "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
    "    gpu_type, count = num_gpus()\n",
    "    # no gpus return cpu\n",
    "    if not gpu_type or count == 0:\n",
    "        return [cpu(),]\n",
    "    return [gpu(i, gpu_type) for i in range(count)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21aa794d-add2-40aa-a11d-a9b58a0c3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding:\n",
    "    \"\"\"Token Embedding.\"\"\"\n",
    "    def __init__(self, embedding_name):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n",
    "            embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in\n",
    "                             enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        data_dir = download_extract(embedding_name)\n",
    "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "        # fastText website: https://fasttext.cc/\n",
    "        with open(os.path.join(data_dir, 'vec.txt'), 'r', encoding='cp437') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
    "                   for token in tokens]\n",
    "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
    "        return vecs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c717e05-3894-4269-8131-9a171442e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "    def avg(self):\n",
    "        \"\"\"Return the average time.\"\"\"\n",
    "        return sum(self.times) / len(self.times)\n",
    "\n",
    "    def sum(self):\n",
    "        \"\"\"Return the sum of time.\"\"\"\n",
    "        return sum(self.times)\n",
    "\n",
    "    def cumsum(self):\n",
    "        \"\"\"Return the accumulated time.\"\"\"\n",
    "        return np.array(self.times).cumsum().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4512b8a6-baea-47c8-b3d6-b635d17c2231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_svg_display():\n",
    "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n",
    "    backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "def set_figsize(figsize=(3.5, 2.5)):\n",
    "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
    "    use_svg_display()\n",
    "    plt.rcParams['figure.figsize'] = figsize\n",
    "\n",
    "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
    "    axes.set_xlabel(xlabel), axes.set_ylabel(ylabel)\n",
    "    axes.set_xscale(xscale), axes.set_yscale(yscale)\n",
    "    axes.set_xlim(xlim),     axes.set_ylim(ylim)\n",
    "    if legend:\n",
    "        axes.legend(legend)\n",
    "    axes.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73565fb9-36a2-4880-8bf3-b65e4daca331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animator:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        use_svg_display()\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # Use a lambda function to capture arguments\n",
    "        self.config_axes = lambda: set_axes(\n",
    "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79903a62-f400-4a9e-8eb7-f3027f15e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b425480-928d-4ee5-a4f3-64fd6676cd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = argmax(y_hat, axis=1)\n",
    "    cmp = astype(y_hat, y.dtype) == y\n",
    "    return float(reduce_sum(astype(cmp, y.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84a05e6a-3d28-4050-8e5e-e6bbdd79eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_in_batch(net, X, y, loss, trainer, devices):\n",
    "    \"\"\"Train for a minibatch with multiple GPUs.\"\"\"\n",
    "    if isinstance(X, list):\n",
    "        # Required for BERT fine-tuning (to be covered later)\n",
    "        X = [x.to(devices[0]) for x in X]\n",
    "    else:\n",
    "        X = X.to(devices[0])\n",
    "    y = y.to(devices[0])\n",
    "    net.train()\n",
    "    trainer.zero_grad()\n",
    "    pred = net(X)\n",
    "    l = loss(pred, y)\n",
    "    l.sum().backward()\n",
    "    trainer.step()\n",
    "    train_loss_sum = l.sum()\n",
    "    train_acc_sum = accuracy(pred, y)\n",
    "    return train_loss_sum, train_acc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "861948e6-a046-4262-87bf-b31eacfd5d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu_for_snli(net, data_iter, device):\n",
    "    \"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # No. of correct predictions, no. of predictions\n",
    "    metric = Accumulator(2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # Required for BERT Fine-tuning (to be covered later)\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)           \n",
    "            metric.add(accuracy(net(X), y), size(y))\n",
    "            \n",
    "    # test_accuracy, no. of predictions, No. of correct predictions, no. of wrong predictions\n",
    "    return (metric[0] / metric[1], metric[1], metric[0], metric[1] - metric[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d40c5465-a5c1-416e-87fa-36025b12c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(net, train_iter, test_iter, loss, trainer, num_epochs, devices):\n",
    "    \"\"\"Train a model with multiple GPUs.\"\"\"\n",
    "    \n",
    "    print(\" ---> started training model ... \")\n",
    "    timer, num_batches = Timer(), len(train_iter)\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n",
    "                            legend=['train loss', 'train accuracy', 'test acccuracy'])\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    for epoch in range(num_epochs):\n",
    "        # Sum of training loss, sum of training accuracy, no. of examples,\n",
    "        # no. of predictions\n",
    "        metric = Accumulator(4)\n",
    "        for i, (features, labels) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            l, acc = train_in_batch(\n",
    "                net, features, labels, loss, trainer, devices)\n",
    "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
    "            timer.stop()\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[2], metric[1] / metric[3],\n",
    "                              None))\n",
    "        test_acc, test_num_of_examples, test_passed, test_failed = evaluate_accuracy_gpu_for_snli(net, test_iter, devices[0])\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "        \n",
    "    test_loss = 1 - test_acc\n",
    "    training, test, results = dict(), dict(), dict()\n",
    "    training['total_examples'], test['total_examples'] = metric[2] , test_num_of_examples\n",
    "    training['passed'], test['passed'] = metric[1], test_passed\n",
    "    training['failed'], test['failed'] = metric[2] - metric[1], test_failed\n",
    "    training['accuracy'], test['accuracy'] = int((metric[1] * 100) / metric[3]), int(test_acc * 100)\n",
    "    training['loss'], test['loss'] = f'{metric[0] / metric[2]:.3f}', f'{test_loss:.3f}'\n",
    "    results['training'] = training\n",
    "    results['test'] = test\n",
    "    print(results)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "777e7c73-dd91-4134-962e-23e3e252113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    \"\"\"The masked language model task of BERT.`\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.LazyLinear(num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.LazyLinear(vocab_size))\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        # Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n",
    "        # `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5980dc8-f390-4f92-96dc-75efc51c6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
    "    # X: 3D tensor, valid_lens: 1D or 2D tensor\n",
    "    def _sequence_mask(X, valid_len, value=0):\n",
    "        maxlen = X.size(1)\n",
    "        mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                            device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = value\n",
    "        return X\n",
    "\n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        # On the last axis, replace masked elements with a very large negative\n",
    "        # value, whose exponentiation outputs 0\n",
    "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72eda387-7bc9-485c-ba4a-6f9303660523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled dot product attention.\"\"\"\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    # Shape of queries: (batch_size, no. of queries, d)\n",
    "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
    "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
    "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        # Swap the last two dimensions of keys with keys.transpose(1, 2)\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a1d2637-35ba-448f-9c8f-7fec1be1a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIHyperParameters:\n",
    "    \"\"\"The base class of hyperparameters.\"\"\"\n",
    "    def save_hyperparameters(self, ignore=[]):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def save_hyperparameters(self, ignore=[]):\n",
    "        \"\"\"Save function arguments into class attributes.`\"\"\"\n",
    "        frame = inspect.currentframe().f_back\n",
    "        _, _, _, local_vars = inspect.getargvalues(frame)\n",
    "        self.hparams = {k:v for k, v in local_vars.items()\n",
    "                        if k not in set(ignore+['self']) and not k.startswith('_')}\n",
    "        for k, v in self.hparams.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dec1ae0-8dc6-4cb2-89a1-a794b8c8d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressBoard(NLIHyperParameters):\n",
    "    \"\"\"The board that plots data points in animation\"\"\"\n",
    "    def __init__(self, xlabel=None, ylabel=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 ls=['-', '--', '-.', ':'], colors=['C0', 'C1', 'C2', 'C3'],\n",
    "                 fig=None, axes=None, figsize=(3.5, 2.5), display=True):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def draw(self, x, y, label, every_n=1):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def draw(self, x, y, label, every_n=1):\n",
    "        Point = collections.namedtuple('Point', ['x', 'y'])\n",
    "        if not hasattr(self, 'raw_points'):\n",
    "            self.raw_points = collections.OrderedDict()\n",
    "            self.data = collections.OrderedDict()\n",
    "        if label not in self.raw_points:\n",
    "            self.raw_points[label] = []\n",
    "            self.data[label] = []\n",
    "        points = self.raw_points[label]\n",
    "        line = self.data[label]\n",
    "        points.append(Point(x, y))\n",
    "        if len(points) != every_n:\n",
    "            return\n",
    "        mean = lambda x: sum(x) / len(x)\n",
    "        line.append(Point(mean([p.x for p in points]),\n",
    "                          mean([p.y for p in points])))\n",
    "        points.clear()\n",
    "        if not self.display:\n",
    "            return\n",
    "        use_svg_display()\n",
    "        if self.fig is None:\n",
    "            self.fig = plt.figure(figsize=self.figsize)\n",
    "        plt_lines, labels = [], []\n",
    "        for (k, v), ls, color in zip(self.data.items(), self.ls, self.colors):\n",
    "            plt_lines.append(plt.plot([p.x for p in v], [p.y for p in v],\n",
    "                                          linestyle=ls, color=color)[0])\n",
    "            labels.append(k)\n",
    "        axes = self.axes if self.axes else plt.gca()\n",
    "        if self.xlim: axes.set_xlim(self.xlim)\n",
    "        if self.ylim: axes.set_ylim(self.ylim)\n",
    "        if not self.xlabel: self.xlabel = self.x\n",
    "        axes.set_xlabel(self.xlabel)\n",
    "        axes.set_ylabel(self.ylabel)\n",
    "        axes.set_xscale(self.xscale)\n",
    "        axes.set_yscale(self.yscale)\n",
    "        axes.legend(plt_lines, labels)\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc9f3e26-4e76-40a0-a86f-458e01c7960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLIModule(nn.Module, NLIHyperParameters):\n",
    "    \"\"\"The base class of models\"\"\"\n",
    "    def __init__(self, plot_train_per_epoch=2, plot_valid_per_epoch=1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.board = ProgressBoard()\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X):\n",
    "        assert hasattr(self, 'net'), 'Neural network is defined'\n",
    "        return self.net(X)\n",
    "\n",
    "    def plot(self, key, value, train):\n",
    "        \"\"\"Plot a point in animation.\"\"\"\n",
    "        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n",
    "        self.board.xlabel = 'epoch'\n",
    "        if train:\n",
    "            x = self.trainer.train_batch_idx / \\\n",
    "                self.trainer.num_train_batches\n",
    "            n = self.trainer.num_train_batches / \\\n",
    "                self.plot_train_per_epoch\n",
    "        else:\n",
    "            x = self.trainer.epoch + 1\n",
    "            n = self.trainer.num_val_batches / \\\n",
    "                self.plot_valid_per_epoch\n",
    "        self.board.draw(x, nlinumpy(nlito(value, cpu())),\n",
    "                        ('train_' if train else 'val_') + key,\n",
    "                        every_n=int(n))\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=True)\n",
    "        return l\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def apply_init(self, inputs, init=None):\n",
    "        self.forward(*inputs)\n",
    "        if init is not None:\n",
    "            self.net.apply(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e93e93e1-ee54-4f16-a270-0ee25fd4a87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(NLIModule):\n",
    "    \"\"\"Multi-head attention.\"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        # Shape of queries, keys, or values:\n",
    "        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n",
    "        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "        # After transposing, shape of output queries, keys, or values:\n",
    "        # (batch_size * num_heads, no. of queries or key-value pairs,\n",
    "        # num_hiddens / num_heads)\n",
    "        queries = self.transpose_qkv(self.W_q(queries))\n",
    "        keys = self.transpose_qkv(self.W_k(keys))\n",
    "        values = self.transpose_qkv(self.W_v(values))\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # On axis 0, copy the first item (scalar or vector) for num_heads\n",
    "            # times, then copy the next item, and so on\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        # Shape of output: (batch_size * num_heads, no. of queries,\n",
    "        # num_hiddens / num_heads)\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        # Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n",
    "        output_concat = self.transpose_output(output)\n",
    "        return self.W_o(output_concat)\n",
    "        \n",
    "    def transpose_qkv(self, X):\n",
    "        \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
    "        # Shape of input X: (batch_size, no. of queries or key-value pairs,\n",
    "        # num_hiddens). Shape of output X: (batch_size, no. of queries or\n",
    "        # key-value pairs, num_heads, num_hiddens / num_heads)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
    "        # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n",
    "        # pairs, num_hiddens / num_heads)\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        # Shape of output: (batch_size * num_heads, no. of queries or key-value\n",
    "        # pairs, num_hiddens / num_heads)\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3])\n",
    "\n",
    "    def transpose_output(self, X):\n",
    "        \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
    "        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
    "        X = X.permute(0, 2, 1, 3)\n",
    "        return X.reshape(X.shape[0], X.shape[1], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e800ece5-9a25-40e2-b367-d4c62074f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    \"\"\"The residual connection followed by layer normalization.\"\"\"\n",
    "    def __init__(self, norm_shape, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(norm_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1649c966-5853-487f-b688-bdb7d1e43c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"The positionwise feed-forward network.`\"\"\"\n",
    "    def __init__(self, ffn_num_hiddens, ffn_num_outputs):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.LazyLinear(ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d604854-9c64-454a-839c-18e821d36f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"The Transformer encoder block.\"\"\"\n",
    "    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,\n",
    "                 use_bias=False):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(num_hiddens, num_heads,\n",
    "                                                dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(num_hiddens, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(num_hiddens, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e090fbc-dac1-4bca-8e26-0547eededcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"Get tokens of the BERT input sequence and their segment IDs.\"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0 and 1 are marking segment A and B, respectively\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "868f1887-d9cf-472b-8259-571d8a6de4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT encoder.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                 num_blks, dropout, max_len=1000, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f'{i}', TransformerEncoderBlock(\n",
    "                num_hiddens, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # In BERT, positional embeddings are learnable, thus we create a\n",
    "        # parameter of positional embeddings that are long enough\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
    "                                                      num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # Shape of `X` remains unchanged in the following code snippet:\n",
    "        # (batch size, max sequence length, `num_hiddens`)\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83138d57-76e0-4b0a-8c86-7b2033735e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"The next sentence prediction task of BERT.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.LazyLinear(2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # `X` shape: (batch size, `num_hiddens`)\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bd70367-95ef-47e3-bd77-9a448686dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    \"\"\"The BERT model.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                 num_heads, num_blks, dropout, max_len=1000):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                                   num_heads, num_blks, dropout,\n",
    "                                   max_len=max_len)\n",
    "        self.hidden = nn.Sequential(nn.LazyLinear(num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens)\n",
    "        self.nsp = NextSentencePred()\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # The hidden layer of the MLP classifier for next sentence prediction.\n",
    "        # 0 is the index of the '<cls>' token\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad6de746-239d-41ca-bcd4-9d22fd8a9ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNLIBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        all_premise_hypothesis_tokens = [[p_tokens, h_tokens] for p_tokens, h_tokens in zip(*[tokenize([s.lower() for s in sentences]) for sentences in dataset[:2]])]\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "        self.valid_lens) = self._preprocess(all_premise_hypothesis_tokens)\n",
    "        print(' ---> read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "    def _preprocess(self, all_premise_hypothesis_tokens):\n",
    "        pool = multiprocessing.Pool(4) # Use 4 worker processes\n",
    "        out = pool.map(self._mp_worker, all_premise_hypothesis_tokens)\n",
    "        all_token_ids = [token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long), torch.tensor(all_segments, dtype=torch.long), torch.tensor(valid_lens))\n",
    "    def _mp_worker(self, premise_hypothesis_tokens):\n",
    "        p_tokens, h_tokens = premise_hypothesis_tokens\n",
    "        self._truncate_pair_of_tokens(p_tokens, h_tokens)\n",
    "        tokens, segments = get_tokens_and_segments(p_tokens, h_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "        * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "    def _truncate_pair_of_tokens(self, p_tokens, h_tokens):\n",
    "        # Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT\n",
    "        # input\n",
    "        while len(p_tokens) + len(h_tokens) > self.max_len - 3:\n",
    "            if len(p_tokens) > len(h_tokens):\n",
    "                p_tokens.pop()\n",
    "            else:\n",
    "                h_tokens.pop()\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx], self.all_segments[idx], self.valid_lens[idx]), self.labels[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "762df892-c06d-42f4-97b4-2965981fa2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        self.output = nn.LazyLinear(3)\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61b11681-4ff2-4237-996c-096e3ea9ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bert(net, vocab, premise, hypothesis):\n",
    "    \"\"\"Predict the logical relationship between the premise and hypothesis.\"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "    device = next(iter(net.parameters())).device\n",
    "    premise_tokens =  premise # [elem.split('.')[0] if '.' in elem else elem for elem in premise.rstrip().split(' ')] + ['.']\n",
    "    hypothesis_tokens =  hypothesis # [elem.split('.')[0] if '.' in elem else elem for elem in hypothesis.rstrip().split(' ')] + ['.']\n",
    "    tokens, segments = get_tokens_and_segments(premise_tokens, hypothesis_tokens)\n",
    "    token_ids = torch.tensor(vocab[tokens], device=device).unsqueeze(0)\n",
    "    segments = torch.tensor(segments, device=device).unsqueeze(0)\n",
    "    valid_len = torch.tensor(len(tokens), device=device).unsqueeze(0)\n",
    "    label = torch.argmax(net((token_ids,segments,valid_len)), dim=1)\n",
    "    return 'entailment' if label == 0 else 'contradiction' if label == 1 else 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7471c275-8794-4f61-917b-b49e3f2839df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(data, mode=None):\n",
    "    if mode:\n",
    "        _mode = data[mode]\n",
    "        return [_mode['total_examples'], _mode['passed'], _mode['failed'], _mode['accuracy'], _mode['loss']]\n",
    "    else:\n",
    "        training = data['training']\n",
    "        test = data['test']\n",
    "        return [ [\"Training\", training['total_examples'], training['passed'], training['failed'], f\"{training['accuracy']}%\", training['loss'] ],\n",
    "                 [\"Test\", test['total_examples'], test['passed'], test['failed'], f\"{test['accuracy']}%\", test['loss']] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d727c19-0c7f-43b0-857c-32770bd937c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_kb(x, pos):\n",
    "    \"\"\"Formats the y-axis values to K.\"\"\"\n",
    "    return f'{x // 1000} K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e95ff1e9-46a6-4e8c-9cc2-6ec9ca65d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(data, title=\"notitle\"):\n",
    "    tabular_data = format_data(data)\n",
    "    training_data = format_data(data, 'training')\n",
    "    test_data = format_data(data, 'test')\n",
    "    columns = [\"Mode\", \"Total examples\", \"Passed\", \"Failed\", \"Accuracy\", \"Loss\"]\n",
    "    modes = [\"Training\", \"Test\"]\n",
    "    subgroups = [\"Passed\", \"Failed\"]\n",
    "    colors = [ [\"#FFFF00\",\"w\",\"w\",\"w\",\"w\", \"w\"], [\"#FFFF00\",\"w\",\"w\",\"w\",\"w\", \"w\"]]\n",
    "    df = pd.DataFrame(tabular_data, columns=columns)\n",
    "\n",
    "    fig = plt.figure(figsize=(8.5,5))\n",
    "\n",
    "    for i in range(1):\n",
    "        ax = plt.subplot2grid((2,2), (0,2*i), colspan=2)\n",
    "        # Hide axes\n",
    "        fig.patch.set_visible(False)\n",
    "        ax.axis('off')\n",
    "        ax.axis('tight')\n",
    "        table = ax.table(cellText=df.values, colLabels=df.columns, loc='center', cellLoc='center', cellColours=colors)\n",
    "        for (row, col), cell in table.get_celld().items():\n",
    "            if (row == 0) or (col == -1):\n",
    "                cell.set_text_props(fontproperties=FontProperties(weight='bold'))\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    row2 = 5\n",
    "    # Define bar width and positions\n",
    "    bar_width = .20\n",
    "    x_positions = np.arange(1)\n",
    "    normalized_training_data = [training_data[1], training_data[2]]\n",
    "    normalized_test_data = [test_data[1], test_data[2]]\n",
    "    for j in range(row2-1):\n",
    "        ax = plt.subplot2grid((2,2*row2), (1,2*j+1), colspan=2)\n",
    "        if j == 0:\n",
    "            for i, subgroup in enumerate(subgroups):\n",
    "                ax.bar(x_positions + i * bar_width, normalized_training_data[i],bar_width, label=subgroup)\n",
    "            ax.set_xticks(x_positions + bar_width / 2)\n",
    "            ax.set_xticklabels([\"Training\"])\n",
    "            ax.yaxis.set_major_formatter(tkr.FuncFormatter(format_kb))\n",
    "        elif j == 1:\n",
    "            for i, subgroup in enumerate(subgroups):\n",
    "                ax.bar(x_positions + i * bar_width, normalized_test_data[i],bar_width, label=subgroup)\n",
    "            # Customize the plot\n",
    "            ax.set_xticks(x_positions + bar_width / 2)\n",
    "            ax.set_xticklabels([\"Test\"])\n",
    "            ax.yaxis.set_major_formatter(tkr.FuncFormatter(format_kb))\n",
    "        elif j == 2:\n",
    "            ax.pie([training_data[1], training_data[2]], labels=[\"passed\", \"failed\"], autopct='%1.1f%%', startangle=90)\n",
    "            ax.set_title('Training')\n",
    "        else:\n",
    "            ax.pie([test_data[1], test_data[2]], labels=[\"passed\", \"failed\"], autopct='%1.1f%%', startangle=90)\n",
    "            ax.set_title('Test') \n",
    "        \n",
    "    # Adjust layout to prevent overlapping titles\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'images/{title}_results.png')\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47ddb30f-18c6-43ef-bac9-ffff5f2844f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(glove, bert_base, bert_small, mode=\"test\"):\n",
    "    title = f'Comparsion of {mode} results accross GloVe, BERT.Base, Bert.Small embeddings'\n",
    "    \n",
    "    glove_data = format_data(glove, mode)\n",
    "    bert_base_data = format_data(bert_base, mode)\n",
    "    bert_small_data = format_data(bert_small, mode)\n",
    "\n",
    "    columns = [\"Embeddings Type\", \"Total examples\", \"Passed\", \"Failed\", \"Accuracy\", \"Loss\"]\n",
    "    embeedings = [\"GloVe\", \"BERT.Base\", \"BERT.Small\"]\n",
    "    labels = [\"Passed\", \"Failed\"]\n",
    "    ccolors = [ [\"#FFFF00\",\"w\",\"w\",\"w\",\"w\", \"w\"], [\"#FFFF00\",\"w\",\"w\",\"w\",\"w\", \"w\"], [\"#FFFF00\",\"w\",\"w\",\"w\",\"w\", \"w\"]]\n",
    "    row1, row2 = 1,4\n",
    "    bar_width = .20\n",
    "    x_positions = np.arange(2)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "    for i in range(row1):\n",
    "        ax = plt.subplot2grid((2,2*row1), (0,2*i), colspan=2)\n",
    "        df = pd.DataFrame([[\"GloVe\"] + glove_data, [\"BERT.Base\"] + bert_base_data, [\"BERT.Small\"] + bert_small_data], columns=columns)\n",
    "        fig.patch.set_visible(False)\n",
    "        ax.axis('off')\n",
    "        ax.axis('tight')\n",
    "        table = ax.table(cellText=df.values, colLabels=df.columns, loc='center', cellLoc='center', cellColours=ccolors)\n",
    "        ax.set_title(title)\n",
    "        for (row, col), cell in table.get_celld().items():\n",
    "            if (row == 0) or (col == -1):\n",
    "                cell.set_text_props(fontproperties=FontProperties(weight='bold'))\n",
    "    \n",
    "    for i in range(row2-1):\n",
    "        ax = plt.subplot2grid((2,2*row2), (1,2*i+1), colspan=2)\n",
    "        if i == 0:\n",
    "            data = [ [glove_data[1], glove_data[2]] , [ bert_base_data[1], bert_base_data[2] ], [bert_small_data[1], bert_small_data[2]]]\n",
    "\n",
    "            for j, subgroup in enumerate(embeedings):\n",
    "                ax.bar(x_positions + j * bar_width, data[j], bar_width, label=subgroup)\n",
    "            ax.set_xticks(x_positions + bar_width / 2)\n",
    "            ax.set_xticklabels(labels)\n",
    "            ax.yaxis.set_major_formatter(tkr.FuncFormatter(format_kb))\n",
    "            ax.legend()\n",
    "        elif i == 1: # pie passed\n",
    "            ax.pie([glove_data[1], bert_base_data[1], bert_small_data[1]], labels=embeedings, autopct='%1.1f%%', startangle=90)\n",
    "            ax.set_title('Passed')\n",
    "        else: # pie failed\n",
    "            ax.pie([glove_data[2], bert_base_data[2], bert_small_data[2]], labels=embeedings, autopct='%1.1f%%', startangle=90)\n",
    "            ax.set_title('Failed')\n",
    "            \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'images/{title}_results.png')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4406759f-aec5-4fdf-acd6-5659cffb1f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_glove(net, vocab, premise, hypothesis):\n",
    "    \"\"\"Predict the logical relationship between the premise and hypothesis.\"\"\"\n",
    "    net.eval()\n",
    "    premise = torch.tensor(vocab[premise], device=try_gpu())\n",
    "    hypothesis = torch.tensor(vocab[hypothesis], device= try_gpu())\n",
    "    label = torch.argmax(net([premise.reshape((1, -1)), hypothesis.reshape((1, -1))]), dim=1)\n",
    "    return 'entailment' if label == 0 else 'contradiction' if label == 1 else 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ece9f90e-2900-4d18-8774-57243514baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_testing(glove_net, glove_vocab, bert_small_net, bert_small_vocab, bert_base_net, bert_base_vocab):\n",
    "    unit_test_data = [\n",
    "        {\n",
    "            'Premise:': 'A man is running the coding example.',\n",
    "            'Hypothesis:': 'The man is sleeping.',\n",
    "            'Expected Result': 'contradiction',\n",
    "            'Glove Emeddings Actual Result': predict_glove(glove_net, glove_vocab, ['A', 'man', 'is', 'running', 'code', 'example', '.'], ['The', 'man', 'is', 'sleeping', '.']),\n",
    "            'Bert.Small Embedings Actual Result': predict_bert(bert_small_net, bert_small_vocab, ['A', 'man', 'is', 'running', 'code', 'example', '.'], ['The', 'man', 'is', 'sleeping', '.']),\n",
    "            'Bert.Base Embedings Actual Result': predict_bert(bert_base_net, bert_base_vocab, ['A', 'man', 'is', 'running', 'code', 'example', '.'], ['The', 'man', 'is', 'sleeping', '.'])\n",
    "        },\n",
    "        {\n",
    "            'Premise:': 'I do need sleep.',\n",
    "            'Hypothesis:': 'I am tired',\n",
    "            'Expected Result': 'entilement',\n",
    "            'Glove Emeddings Actual Result': predict_glove(glove_net, glove_vocab,  ['I', 'do', 'need', 'sleep', '.'], ['I', 'iam', 'tired', '.']),\n",
    "            'Bert.Small Embedings Actual Result': predict_bert(bert_small_net, bert_small_vocab,  ['I', 'do', 'need', 'sleep', '.'], ['I', 'iam', 'tired', '.']),\n",
    "            'Bert.Base Embedings Actual Result': predict_bert(bert_base_net, bert_base_vocab,  ['I', 'do', 'need', 'sleep', '.'], ['I', 'iam', 'tired', '.'])\n",
    "        },\n",
    "        {\n",
    "            'Premise:': 'The musicians are performing for us.',\n",
    "            'Hypothesis:': 'The musicians are famous.',\n",
    "            'Expected Result': 'neutral',\n",
    "            'Glove Emeddings Actual Result': predict_glove(glove_net, glove_vocab, ['The', 'musicians', 'are', 'performing', 'for', 'us', '.'], ['The', 'musicians', 'are', 'famous', '.']),\n",
    "            'Bert.Small Embedings Actual Result': predict_bert(bert_small_net, bert_small_vocab, ['The', 'musicians', 'are', 'performing', 'for', 'us', '.'], ['The', 'musicians', 'are', 'famous', '.']),\n",
    "            'Bert.Base Embedings Actual Result': predict_bert(bert_base_net, bert_base_vocab, ['The', 'musicians', 'are', 'performing', 'for', 'us', '.'], ['The', 'musicians', 'are', 'famous', '.'])\n",
    "        }\n",
    "    ]\n",
    "    formatted_unit_test_data = json.dumps(unit_test_data, indent=4)\n",
    "    print(\"Initial testing results\")\n",
    "    print(formatted_unit_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e112b3-5138-4656-8dd5-9228767228a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
